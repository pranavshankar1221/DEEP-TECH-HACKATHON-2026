{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd93be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 0.5161 | Train Acc: 83.16% | Val Loss: 0.1953 | Val Acc: 93.16%\n",
      "Epoch [2/20] | Train Loss: 0.1947 | Train Acc: 93.49% | Val Loss: 0.1944 | Val Acc: 93.02%\n",
      "Epoch [3/20] | Train Loss: 0.1220 | Train Acc: 96.00% | Val Loss: 0.0855 | Val Acc: 96.87%\n",
      "Epoch [4/20] | Train Loss: 0.0811 | Train Acc: 97.88% | Val Loss: 0.0731 | Val Acc: 97.72%\n",
      "Epoch [5/20] | Train Loss: 0.0625 | Train Acc: 98.04% | Val Loss: 0.1216 | Val Acc: 96.30%\n",
      "Epoch [6/20] | Train Loss: 0.0543 | Train Acc: 98.38% | Val Loss: 0.0606 | Val Acc: 98.01%\n",
      "Epoch [7/20] | Train Loss: 0.0450 | Train Acc: 98.63% | Val Loss: 0.0515 | Val Acc: 98.43%\n",
      "Epoch [8/20] | Train Loss: 0.0351 | Train Acc: 98.97% | Val Loss: 0.0808 | Val Acc: 97.58%\n",
      "Epoch [9/20] | Train Loss: 0.0295 | Train Acc: 99.05% | Val Loss: 0.0396 | Val Acc: 98.58%\n",
      "Epoch [10/20] | Train Loss: 0.0290 | Train Acc: 99.18% | Val Loss: 0.0400 | Val Acc: 98.86%\n",
      "Epoch [11/20] | Train Loss: 0.0194 | Train Acc: 99.36% | Val Loss: 0.0559 | Val Acc: 98.15%\n",
      "Epoch [12/20] | Train Loss: 0.0223 | Train Acc: 99.30% | Val Loss: 0.0450 | Val Acc: 98.15%\n",
      "Epoch [13/20] | Train Loss: 0.0206 | Train Acc: 99.32% | Val Loss: 0.0351 | Val Acc: 98.86%\n",
      "Epoch [14/20] | Train Loss: 0.0204 | Train Acc: 99.32% | Val Loss: 0.0400 | Val Acc: 98.58%\n",
      "Epoch [15/20] | Train Loss: 0.0170 | Train Acc: 99.52% | Val Loss: 0.0305 | Val Acc: 98.86%\n",
      "Epoch [16/20] | Train Loss: 0.0199 | Train Acc: 99.38% | Val Loss: 0.0765 | Val Acc: 97.86%\n",
      "Epoch [17/20] | Train Loss: 0.0392 | Train Acc: 98.84% | Val Loss: 0.0760 | Val Acc: 97.86%\n",
      "Epoch [18/20] | Train Loss: 0.0173 | Train Acc: 99.57% | Val Loss: 0.0434 | Val Acc: 98.86%\n",
      "Epoch [19/20] | Train Loss: 0.0129 | Train Acc: 99.54% | Val Loss: 0.0598 | Val Acc: 98.29%\n",
      "Epoch [20/20] | Train Loss: 0.0185 | Train Acc: 99.34% | Val Loss: 0.0504 | Val Acc: 98.86%\n",
      "âœ… Training done. Best Val Acc: 98.86039886039886\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ TRAIN + VALIDATE + TEST CUSTOM CNN (GRAYSCALE FIXED)\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "# ---------------------------- Settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 20           # Increase epochs for better learning\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 64           # Match your model input\n",
    "\n",
    "data_dir = r\"D:\\DeepTech\\data\"\n",
    "train_folder = os.path.join(data_dir, \"train\")\n",
    "val_folder   = os.path.join(data_dir, \"valid\")\n",
    "test_folder  = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# ---------------------------- Data Loaders (Grayscale + Augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to 1 channel\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to 1 channel\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=train_transforms)\n",
    "val_dataset   = datasets.ImageFolder(val_folder, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------------------------- Define Custom CNN (1-channel input)\n",
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # 1 channel\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)      # IMAGE_SIZE/4 = 16\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyCustomCNN().to(device)\n",
    "\n",
    "# ---------------------------- Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_val_acc = 0\n",
    "\n",
    "# ---------------------------- Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    correct, total, running_loss = 0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    correct, total, val_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"class_names\": class_names,\n",
    "            \"val_acc\": val_acc\n",
    "        }, \"best_cnn_phase1.pth\")\n",
    "\n",
    "print(\"âœ… Training done. Best Val Acc:\", best_val_acc)\n",
    "\n",
    "# ---------------------------- Test Function\n",
    "def predict_image(img_path, model, class_names):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"L\")  # Ensure 1 channel\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        max_prob, idx = torch.max(probs, 0)\n",
    "    print(f\"Image: {os.path.basename(img_path)} | \"\n",
    "          f\"Predicted Class: {class_names[idx]} | Confidence: {max_prob.item()*100:.2f}%\")\n",
    "\n",
    "# ---------------------------- Test All Images in Test Folder\n",
    "checkpoint = torch.load(\"best_cnn_phase1.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "for img_path in glob.glob(os.path.join(test_folder, \"*.*\")):\n",
    "    predict_image(img_path, model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a58a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: image_LOC_32014_png_jpg.rf.291c9481f3abfc2a417e7b99b3cea11a.jpg\n",
      "Predicted Class: Local\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ TEST SINGLE IMAGE\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ---------------------------- Settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 64  # Must match training\n",
    "img_path = r\"D:\\DeepTech\\data\\train\\Local\\image_LOC_32014_png_jpg.rf.291c9481f3abfc2a417e7b99b3cea11a.jpg\"\n",
    "\n",
    "# ---------------------------- Load checkpoint\n",
    "checkpoint = torch.load(\"best_cnn_phase1.pth\", map_location=device)\n",
    "class_names = checkpoint['class_names']\n",
    "\n",
    "# Define the same model architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=len(class_names)):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # grayscale input\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and load weights\n",
    "model = MyCustomCNN().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------- Prediction\n",
    "def predict_single_image(img_path, model, class_names):\n",
    "    img = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        max_prob, idx = torch.max(probs, 0)\n",
    "    print(f\"Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"Predicted Class: {class_names[idx]}\")\n",
    "    print(f\"Confidence: {max_prob.item() * 100:.2f}%\")\n",
    "\n",
    "# Run prediction\n",
    "predict_single_image(img_path, model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bed1e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: image_Center_12009_png_jpg.rf.f507dcdd0e47b802409836643343b63d.jpg | Predicted Class: Center | Confidence: 99.99%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ TEST IMAGE WITH INVALID CHECK\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 64\n",
    "CONFIDENCE_THRESHOLD = 0.7  # 70% threshold\n",
    "\n",
    "# Load saved model checkpoint\n",
    "checkpoint = torch.load(\"best_cnn_phase1.pth\", map_location=device)\n",
    "class_names = checkpoint['class_names']\n",
    "\n",
    "# Define the CNN (same as training)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=len(class_names)):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and load weights\n",
    "model = MyCustomCNN().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------- Prediction function with invalid check\n",
    "def predict_image_with_invalid(img_path, model, class_names, threshold=CONFIDENCE_THRESHOLD):\n",
    "    img = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        max_prob, idx = torch.max(probs, 0)\n",
    "\n",
    "    if max_prob.item() < threshold:\n",
    "        print(f\"Image: {os.path.basename(img_path)} | Prediction: INVALID / Unknown image\")\n",
    "    else:\n",
    "        print(f\"Image: {os.path.basename(img_path)} | Predicted Class: {class_names[idx]} | Confidence: {max_prob.item()*100:.2f}%\")\n",
    "\n",
    "# ---------------------------- Test single or multiple images\n",
    "test_images = [\n",
    "    r\"D:\\DeepTech\\data\\train\\Center\\image_Center_12009_png_jpg.rf.f507dcdd0e47b802409836643343b63d.jpg\"  # unrelated image\n",
    "]\n",
    "\n",
    "for img_path in test_images:\n",
    "    predict_image_with_invalid(img_path, model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8c12138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0208 11:01:30.739000 3752 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0208 11:01:31.357000 3752 site-packages\\torch\\onnx\\_internal\\exporter\\_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0208 11:01:31.360000 3752 site-packages\\torch\\onnx\\_internal\\exporter\\_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0208 11:01:31.363000 3752 site-packages\\torch\\onnx\\_internal\\exporter\\_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
      "W0208 11:01:31.369000 3752 site-packages\\torch\\onnx\\_internal\\exporter\\_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MyCustomCNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MyCustomCNN([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 11).\n",
      "Failed to convert the model to the target version 11 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxscript\\version_converter\\_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnx\\version_converter.py\", line 37, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: D:\\a\\onnx\\onnx\\onnx\\onnx/version_converter/BaseConverter.h:73: adapter_lookup: Assertion `false` failed: No Adapter From Version $14 for Relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "âœ… Model exported to ONNX: wafer_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"best_cnn_phase1.pth\", map_location=\"cpu\")\n",
    "class_names = checkpoint['class_names']\n",
    "\n",
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=len(class_names)):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyCustomCNN()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 1, 64, 64)\n",
    "onnx_file = \"wafer_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model, dummy_input, onnx_file,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11\n",
    ")\n",
    "print(f\"âœ… Model exported to ONNX: {onnx_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88fa14b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: wafer_model_tf\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m converter = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFLiteConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwafer_model_tf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\u001b[32m      5\u001b[39m tflite_model = converter.convert()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:2266\u001b[39m, in \u001b[36mTFLiteConverterV2.from_saved_model\u001b[39m\u001b[34m(cls, saved_model_dir, signature_keys, tags)\u001b[39m\n\u001b[32m   2263\u001b[39m   tags = \u001b[38;5;28mset\u001b[39m([_tag_constants.SERVING])\n\u001b[32m   2265\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context.eager_mode():\n\u001b[32m-> \u001b[39m\u001b[32m2266\u001b[39m   saved_model = \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signature_keys:\n\u001b[32m   2268\u001b[39m   signature_keys = \u001b[38;5;28mlist\u001b[39m(saved_model.signatures.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:912\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(export_dir, tags, options)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os.PathLike):\n\u001b[32m    911\u001b[39m   export_dir = os.fspath(export_dir)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m result = \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mroot\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:1016\u001b[39m, in \u001b[36mload_partial\u001b[39m\u001b[34m(export_dir, filters, tags, options)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, \u001b[38;5;28mset\u001b[39m):\n\u001b[32m   1012\u001b[39m   \u001b[38;5;66;03m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[32m   1013\u001b[39m   \u001b[38;5;66;03m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[32m   1014\u001b[39m   tags = nest.flatten(tags)\n\u001b[32m   1015\u001b[39m saved_model_proto, debug_info = (\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     \u001b[43mloader_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_saved_model_with_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1018\u001b[39m loader = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(saved_model_proto.meta_graphs) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1020\u001b[39m     saved_model_proto.meta_graphs[\u001b[32m0\u001b[39m].HasField(\u001b[33m\"\u001b[39m\u001b[33mobject_graph_def\u001b[39m\u001b[33m\"\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:59\u001b[39m, in \u001b[36mparse_saved_model_with_debug_info\u001b[39m\u001b[34m(export_dir)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[32m     47\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \u001b[33;03m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m   saved_model = \u001b[43mparse_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m   debug_info_path = file_io.join(\n\u001b[32m     62\u001b[39m       path_helpers.get_debug_dir(export_dir),\n\u001b[32m     63\u001b[39m       constants.DEBUG_INFO_FILENAME_PB)\n\u001b[32m     64\u001b[39m   debug_info = graph_debug_info_pb2.GraphDebugInfo()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pranav shankar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:119\u001b[39m, in \u001b[36mparse_saved_model\u001b[39m\u001b[34m(export_dir)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot parse file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_pbtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[32m    120\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSavedModel file does not exist at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mos.path.sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconstants.SAVED_MODEL_FILENAME_PBTXT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants.SAVED_MODEL_FILENAME_PB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m saved_model\n",
      "\u001b[31mOSError\u001b[39m: SavedModel file does not exist at: wafer_model_tf\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"wafer_model_tf\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"wafer_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"âœ… Model converted to TFLite: wafer_model.tflite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
